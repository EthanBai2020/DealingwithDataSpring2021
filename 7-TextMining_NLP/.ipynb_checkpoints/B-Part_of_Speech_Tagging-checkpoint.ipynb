{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing and Tagging Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These are  very useful categories for many language processing tasks. Our goals chapter is to answer the following questions:\n",
    "\n",
    "1. What are lexical categories and how are they used in natural language processing?\n",
    "2. What is a good Python data structure for storing words and their categories?\n",
    "3. How can we automatically tag each word of a text with its word class?\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:\n",
    "This is very important for trying to extract meaning from text. We often  need to find out the WHAT, WHERE, WHO and HOW in a document, or determine the sentiment of a document. The NLTK (Natural Language Tool Kit) library is one of a number of systems that we can use to understand text. Here are some examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the toolkit\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the toolkit to tokenize (parse)some text into words, and then label the words with their parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "# Show the parts of speech for each word\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but what do 'CC', 'RB', 'IN', mean? Here we see that AND  is a CC, a coordinating conjunction; NOW and COMPLETELY are RB, or adverbs; FOR is an IN, a preposition; SOMETHING is NN, a noun; and DIFFERENT is JJ, an adjective.\n",
    "\n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`, or a regular expression, e.g. `nltk.help.upenn_tagset('NN.*')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the next command doesn't work, type nltk.download()\n",
    "and download the 'book' grammer, by typing'd' and then 'book'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASK NLTK what a   JJ is, and some examples\n",
    "\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAGSET meanings for the UPENN  (default) tagset.\n",
    "Display all of the possible POS tags and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Tagged Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention in NLTK, a tagged token is represented using a **tuple** consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token, using the function str2tuple():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how NLTK treats (disambiguates)  the two occurences of the token \"refuse\" in the sentence below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index into the \"tagged\" tuple and retrieve the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_token = tagged[0]\n",
    "print(tagged_token)\n",
    "print(tagged_token[0])\n",
    "print(tagged_token[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets iterate through the tagged tuples and break out the token and the POS Tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the original text, tokenized\n",
    "print(\"Text = \", tokens)\n",
    "# Now the same from the tagged tuples (note the list comprehension)\n",
    "tokens = [a for (a, b) in tagged]\n",
    "print(\"Tokens = \",tokens)\n",
    "# and then print the POS TAGs\n",
    "tags = [b for (a, b) in tagged]\n",
    "print(\"POS Tags = \", tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Load a text of your choice, tokenize it, and perform part of speech tagging on it. Then extract the nouns from the text, and perform a frequency anaysis, to identify the most common nouns in the text. (Warning: POS tagging takes a good amount of time when processing long texts, so try to select a text with less than 10K tokens, or simply perform POS tagging on the first 10K-20K tokens).\n",
    "\n",
    "Repeat the exercise for adjectives.\n",
    "\n",
    "PS: If you want to parse text from HTML without resorting to XPath expressions, you can use the \"BeautifulSoup\" library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.nytimes.com/2017/05/22/world/europe/ariana-grande-manchester-police.html\"\n",
    "#url=\"https://www.nytimes.com/2017/09/01/business/economy/jobs-report-unemployment.html\"\n",
    "resp = requests.get(url)\n",
    "html = resp.text \n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "\n",
    "# The code below is to remove the junk that was extracted in addition to the article\n",
    "start = raw.index(\"MANCHESTER, England â€”\")\n",
    "#start = raw.index(\"MACUNGIE\")\n",
    "end = raw.index(\"Rory Smith reported from Manchester, and Sewell Chan from London\")\n",
    "#end = raw.index(\"Clifford Krauss and Bill Vlasic contributed reporting.\")\n",
    "raw = raw[start:end]\n",
    "\n",
    "# Let's do the NLTK stuff\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nouns from the text\n",
    "nouns = [token for (token,tag) in tagged if  tag.startswith('NN') and token.isalpha()]\n",
    "fd_nyt = nltk.FreqDist(nouns)\n",
    "fd_nyt.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the adjectives from the text\n",
    "adjectives = [token for (token,tag) in tagged if  tag.startswith('JJ')  and token.isalpha()]\n",
    "fd_nyt = nltk.FreqDist(adjectives)\n",
    "fd_nyt.most_common(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Note that the raw text has lots of html tags in it. We would like to do more preprocessing, and eliminate all of the html. The beautiful soup library has much more power than we have shown here, and our code could be customized to pull just the text from New York Times articles, twitter feeds etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
