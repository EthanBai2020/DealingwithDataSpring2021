{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Inaugural Speeches\n",
    "\n",
    "#### Exercise\n",
    "You are asked to identify the words that are most indicative of an Presidential inaugural speech for a given year.\n",
    "For this task, you will have to do the following:\n",
    "* Select the target speeches\n",
    "* Treat each sentence in the target speech as a document; if the sentence is part of the target speeches, mark it as positive, otherwise mark it as negative\n",
    "* Create a dataset that contains the words that appear in each \"positive\" and in each \"negative\" sentence; filter the words so that we only see words that appear in a sufficiently large number of sentences.\n",
    "* Train a classifier\n",
    "* See the most informative words\n",
    "\n",
    "The NLTK toolkit contains the inaugural speeches for all presidents from 1789 till 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the words and/or sentences of these speeches we use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = '2009-Obama.txt'\n",
    "\n",
    "# Here is the list of sentences. Each sentence is a list of tokens\n",
    "inaugural.sents(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = '2001-Bush.txt'\n",
    "\n",
    "# Here is the list of sentences. Each sentence is a list of tokens\n",
    "inaugural.sents(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the first sentence\n",
    "inaugural.sents(speech)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the second sentence\n",
    "inaugural.sents(speech)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is the list of tokens\n",
    "list(inaugural.words(speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# And here is the raw text\n",
    "raw_text = inaugural.raw(speech)\n",
    "\n",
    "# And as a reminder, here are the NTLK commands for \n",
    "# splitting the text into sentences, or tokenizing it\n",
    "# (See part A for more details)\n",
    "sentences = nltk.sent_tokenize(raw_text)\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "nltk_text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the list of (non-tokenized) sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is an example of doing POS tagging on the second sentence\n",
    "sent_tokens = nltk.word_tokenize(sentences[1])\n",
    "nltk.pos_tag(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "You are asked to identify the words that are most indicative of an Presidential inaugural speech for a given year. \n",
    "\n",
    "For this task, you will have to do the following:\n",
    "* Select the target speeches\n",
    "* Treat each sentence in the target speech as a document; if the sentence is part of the target speeches, mark it as positive, otherwise mark it as negative\n",
    "* Create a dataset that contains the words that appear in each \"positive\" and in each \"negative\" sentence; filter the words so that we only see words that appear in a sufficiently large number of sentences.\n",
    "* Train a classifier\n",
    "* See the most informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our \"target\" class. We will define our target class, as all the speeches\n",
    "# in the 18th and 19th century\n",
    "target_speeches = ['1949-Truman.txt',\n",
    " '1953-Eisenhower.txt',\n",
    " '1957-Eisenhower.txt',\n",
    " '1961-Kennedy.txt',\n",
    " '1965-Johnson.txt',\n",
    " '1969-Nixon.txt',\n",
    " '1973-Nixon.txt',\n",
    " '1977-Carter.txt',\n",
    " '1981-Reagan.txt',\n",
    " '1985-Reagan.txt',\n",
    " '1989-Bush.txt',\n",
    " '1993-Clinton.txt',\n",
    " '1997-Clinton.txt',\n",
    " '2001-Bush.txt',\n",
    " '2005-Bush.txt',\n",
    " '2009-Obama.txt']\n",
    "\n",
    "# Or shorter....\n",
    "#target_speeches = [s for s in inaugural.fileids() \n",
    "#                   if s.startswith('17') or s.startswith('18') or s.startswith('1901')]\n",
    "\n",
    "non_target_speeches = [s for s in inaugural.fileids() if s not in target_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We go over all speeches, and extract the sentences (each sentence is a list, containing the words/tokens)\n",
    "# If the speech is a target speech, add the sentence\n",
    "\n",
    "# The data will contain a tuple (\"pos\", sentence) and (\"neg\", sentence)\n",
    "data = []\n",
    "speeches = inaugural.fileids()\n",
    "\n",
    "for speech in speeches:\n",
    "    \n",
    "    if speech in target_speeches:\n",
    "        label = \"pos\"\n",
    "    else:\n",
    "        label = \"neg\"\n",
    "    # If we want to operate with the raw text\n",
    "    raw_text = inaugural.raw(speech)\n",
    "    sentences = nltk.sent_tokenize(raw_text)\n",
    "    # Or, alternatively, to add the alterady tokenized sentences\n",
    "    # sentences = list(inaugural.sents(speech))\n",
    "    \n",
    "    # We now add the sentences in our dataset, with the appropriate tag\n",
    "    # We create a list comprehension for each sentence in the speech\n",
    "    # and then we add all these elements into \"data\"\n",
    "    data.extend( [(label, sent) for sent in sentences] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of positive sentences\n",
    "len([tag for (tag, s) in data if tag=='pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of negative sentences\n",
    "len([tag for (tag, s) in data if tag=='neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is out function that takes as input a sentence and then extracts\n",
    "# the features, and creates the feature dictionary that we will use for\n",
    "# training. We use binary representation of our features (either the feature\n",
    "# appears in the sentence or not). Notice that we only set as \"True\" the \n",
    "# features that appear; the remaining ones will be implicitly set to \"None\"/False\n",
    "def features(sentence):\n",
    "    features = dict()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "    for token, pos_tag in pos_tagged_tokens:\n",
    "        # We keep only specific part of speech as features\n",
    "        #if (pos_tag.startswith(\"J\")):\n",
    "            features[token+\"/\"+pos_tag] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Here is our first data point/sentence\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the featurized version of the sentence\n",
    "features(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, now we go through all elements in the \"data\" list (tag, sentence)\n",
    "# and we apply the \"features\" function in each sentence, to get back its\n",
    "# featurized form\n",
    "featurized_data = [(features(sentence), class_label) \n",
    "                   for (class_label, sentence) in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(featurized_data)\n",
    "test_set_size = 500\n",
    "train_set, test_set = featurized_data[test_set_size:], featurized_data[:test_set_size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
